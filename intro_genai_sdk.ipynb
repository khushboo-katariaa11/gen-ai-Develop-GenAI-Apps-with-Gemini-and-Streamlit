{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Use the Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a Generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-04-dce56f17e5d0\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"qwiklabs-gcp-04-dce56f17e5d0\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T-tiytzQE0uM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6fc324893334",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3PoF18EwhI7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D3SI1X-JVMBj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Your Week, Solved: The Delicious Art of Meal Prep!\n",
      "\n",
      "Ever gaze longingly at perfectly portioned, vibrant meals and think, \"I wish my week looked that organized?\" Well, wonder no more! The image before us is a beautiful testament to the power and practicality of meal prepping, a culinary superhero for anyone looking to simplify their busy schedule without sacrificing health or flavor.\n",
      "\n",
      "Just look at these vibrant glass containers! Each one holds a complete, balanced meal, ready to grab and go. We see fluffy, wholesome rice forming the base, topped with tender, glazed chicken (hello, delicious teriyaki or stir-fry vibes!). Complementing the protein are bright green broccoli florets and strips of orange and red bell peppers, adding a burst of color, fiber, and essential nutrients. A sprinkle of sesame seeds and fresh green onions completes the picture, promising a delightful crunch and fresh aroma.\n",
      "\n",
      "Meal prepping isn't just about aesthetics; it's a true game-changer. Imagine:\n",
      "*   **No more frantic lunch decisions** during a hectic workday.\n",
      "*   **Saving money** by avoiding expensive takeout.\n",
      "*   **Fueling your body** with nutritious, homemade food tailored to your preferences.\n",
      "*   **Reducing food waste** by planning out your ingredients.\n",
      "\n",
      "This particular dish showcases how easy and appealing healthy eating can be. It's packed with lean protein from the chicken, complex carbohydrates from the rice, and a rainbow of vitamins and minerals from the fresh vegetables. Plus, it's designed for convenience – just reheat and enjoy! The glass containers are perfect for storing, heating, and even serving, making your weekday meals a breeze.\n",
      "\n",
      "So, if you're looking to reclaim your time, nourish your body, and bring a little more calm and deliciousness to your week, take inspiration from this inviting spread. Start small, pick a favorite recipe, and unlock the magic of meal prep. Your taste buds (and your schedule) will thank you!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pG6l1Fuka6ZJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Beyond the Brown Bag: This is Your Meal Prep Motivation!\n",
      "\n",
      "Tired of last-minute lunch scrambles or resorting to expensive, unhealthy takeout? Take a look at this vision of organized deliciousness!\n",
      "\n",
      "What you see here are two perfectly portioned glass containers, brimming with the kind of meal that makes you excited for lunchtime. Each one is packed with vibrant, stir-fried goodness: glistening, savory chicken pieces, bright green broccoli florets, and colorful julienned carrots and red bell peppers, all nestled beside a generous serving of fluffy rice. A sprinkle of sesame seeds and fresh green onions adds that extra touch of flavor and visual appeal.\n",
      "\n",
      "This isn't just a pretty picture; it's a testament to the power of meal prepping! Imagine having this ready to grab from your fridge on a busy Monday morning. No more stressing about what to eat, no more compromising on health, and definitely no more wasting money on less-than-satisfying options.\n",
      "\n",
      "Meal prepping offers so many benefits:\n",
      "*   **Time-Saving:** Cook once, eat multiple times!\n",
      "*   **Healthier Choices:** You control the ingredients, portions, and cooking methods.\n",
      "*   **Budget-Friendly:** Eating out less frequently saves significant cash.\n",
      "*   **Stress Reduction:** One less decision to make during a hectic week.\n",
      "\n",
      "This particular meal looks like a fantastic example of a balanced, satisfying, and delicious Asian-inspired stir-fry that holds up beautifully throughout the week. So, if you've been on the fence about trying meal prep, let this colorful, convenient, and oh-so-appetizing image be your sign. Your future self (and your taste buds!) will thank you.\n",
      "\n",
      "Here's to a week of delicious, stress-free, and perfectly prepped meals!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof woof! Okay, little floof, listen up! This is all about **squeaky toys**!\n",
      "\n",
      "Imagine you have your absolute FAVORITE **squeaky toy** – that's like a picture of a squirrel, or a video of a bouncy ball, or just a little \"woof\" message you want to send.\n",
      "\n",
      "1.  **You (Your Computer/Phone):** You're a very good puppy! You have the toy. When you want to send it, you **SQUEAK IT!** (That's like typing or tapping).\n",
      "\n",
      "2.  **The Invisible Leash (Wires) or Invisible Sniffy Path (Wi-Fi):**\n",
      "    *   Sometimes, your squeaky toy goes on a long, long, *invisible leash* straight to another puppy's mouth! (That's like a wire, super fast!)\n",
      "    *   Other times, your squeaky toy just *floats through the air* on an *invisible sniffy path*! You can't see it, but your nose knows it's there! (That's Wi-Fi!)\n",
      "\n",
      "3.  **The Super Smart Squirrel (The Router):**\n",
      "    *   Now, there are SO many puppies in the world, right? How does your squeaky toy know where to go? There's a **super-duper smart squirrel** who lives in your house!\n",
      "    *   When you squeak your toy, the squirrel *sniffs it* and knows *exactly* which invisible leash or sniffy path to send it down to get to the *right* puppy. He's like the best fetch player EVER!\n",
      "\n",
      "4.  **The Giant, Super-Duper Toy Box (Servers):**\n",
      "    *   Sometimes, puppies don't just send toys to *other puppies*. Sometimes, they want to put their toys in a **GIANT, super-duper toy box** where *everyone* can come and look at them!\n",
      "    *   These giant toy boxes are in big, comfy dog beds far, far away. When you want to see *all* the toys about squirrels, you send a little \"woof\" to the giant toy box, and it sends *all* its squirrel toys back to you!\n",
      "\n",
      "5.  **The Special Sniff-Spot (Addresses/Websites):**\n",
      "    *   Every puppy has a **special sniff-spot** – like a unique smell on their collar. That's how the smart squirrel knows *exactly* which puppy to send the toy to.\n",
      "    *   And those giant toy boxes? They have special sniff-spots too! Like \"www.squirrels-are-fun.com\" – that's just a special sniff-spot for the squirrel toy box!\n",
      "\n",
      "So, in short, little floof: The internet is just a HUGE, invisible dog park where **squeaky toys** (your messages, pictures, videos) zoom around on invisible leashes and sniffy paths, guided by super smart squirrels, to get to other puppies or giant toy boxes!\n",
      "\n",
      "Now, go chase that tail! Good boy/girl!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two disrespectful things you might say to the universe after stubbing your toe in the dark:\n",
      "\n",
      "1.  \"Oh, so *this* is your grand plan for me tonight, Universe? A dark hallway and a broken pinky toe? Real original, you cosmic imbecile!\"\n",
      "2.  \"Is this your idea of a cosmic joke?! Because it's not funny, you omniscient jerk! And by the way, your interior decorating skills are terrible – too much *dark*!\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7R7eyEBetsns",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=9.771177e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.042388976\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=7.847613e-05,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.059951603\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.0015537471,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.052799836\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=3.7881887e-06,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.039865077\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're looking for a function to determine if a given year is a leap year. The rules for leap years are specific:\n",
      "\n",
      "1.  A year **is** a leap year if it is evenly divisible by 400.\n",
      "2.  A year **is not** a leap year if it is evenly divisible by 100 but *not* by 400.\n",
      "3.  A year **is** a leap year if it is evenly divisible by 4 but *not* by 100.\n",
      "4.  Otherwise, the year **is not** a leap year.\n",
      "\n",
      "This can be summarized as: a year is a leap year if it's divisible by 400, OR if it's divisible by 4 but not by 100.\n",
      "\n",
      "Here's the function implemented in a few common programming languages:\n",
      "\n",
      "---\n",
      "\n",
      "## Python\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    A year is a leap year if:\n",
      "    1. It is divisible by 400.\n",
      "    OR\n",
      "    2. It is divisible by 4 but not by 100.\n",
      "\n",
      "    Args:\n",
      "        year: The year to check (e.g., 2000, 1996, 1900).\n",
      "\n",
      "    Returns:\n",
      "        True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Example Usage ---\n",
      "print(f\"Is 2000 a leap year? {is_leap_year(2000)}\") # Expected: True (divisible by 400)\n",
      "print(f\"Is 1996 a leap year? {is_leap_year(1996)}\") # Expected: True (divisible by 4, not by 100)\n",
      "print(f\"Is 1900 a leap year? {is_leap_year(1900)}\") # Expected: False (divisible by 100, not by 400)\n",
      "print(f\"Is 2001 a leap year? {is_leap_year(2001)}\") # Expected: False (not divisible by 4)\n",
      "print(f\"Is 2024 a leap year? {is_leap_year(2024)}\") # Expected: True\n",
      "print(f\"Is 2100 a leap year? {is_leap_year(2100)}\") # Expected: False\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## JavaScript\n",
      "\n",
      "```javascript\n",
      "/**\n",
      " * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      " *\n",
      " * A year is a leap year if:\n",
      " * 1. It is divisible by 400.\n",
      " * OR\n",
      " * 2. It is divisible by 4 but not by 100.\n",
      " *\n",
      " * @param {number} year The year to check (e.g., 2000, 1996, 1900).\n",
      " * @returns {boolean} True if the year is a leap year, False otherwise.\n",
      " */\n",
      "function isLeapYear(year) {\n",
      "  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n",
      "}\n",
      "\n",
      "// --- Example Usage ---\n",
      "console.log(`Is 2000 a leap year? ${isLeapYear(2000)}`); // Expected: True\n",
      "console.log(`Is 1996 a leap year? ${isLeapYear(1996)}`); // Expected: True\n",
      "console.log(`Is 1900 a leap year? ${isLeapYear(1900)}`); // Expected: False\n",
      "console.log(`Is 2001 a leap year? ${isLeapYear(2001)}`); // Expected: False\n",
      "console.log(`Is 2024 a leap year? ${isLeapYear(2024)}`); // Expected: True\n",
      "console.log(`Is 2100 a leap year? ${isLeapYear(2100)}`); // Expected: False\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Java\n",
      "\n",
      "```java\n",
      "public class DateUtils {\n",
      "\n",
      "    /**\n",
      "     * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "     *\n",
      "     * A year is a leap year if:\n",
      "     * 1. It is divisible by 400.\n",
      "     * OR\n",
      "     * 2. It is divisible by 4 but not by 100.\n",
      "     *\n",
      "     * @param year The year to check (e.g., 2000, 1996, 1900).\n",
      "     * @return True if the year is a leap year, False otherwise.\n",
      "     */\n",
      "    public static boolean isLeapYear(int year) {\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "\n",
      "    // --- Example Usage ---\n",
      "    public static void main(String[] args) {\n",
      "        System.out.println(\"Is 2000 a leap year? \" + isLeapYear(2000)); // Expected: True\n",
      "        System.out.println(\"Is 1996 a leap year? \" + isLeapYear(1996)); // Expected: True\n",
      "        System.out.println(\"Is 1900 a leap year? \" + isLeapYear(1900)); // Expected: False\n",
      "        System.out.println(\"Is 2001 a leap year? \" + isLeapYear(2001)); // Expected: False\n",
      "        System.out.println(\"Is 2024 a leap year? \" + isLeapYear(2024)); // Expected: True\n",
      "        System.out.println(\"Is 2100 a leap year? \" + isLeapYear(2100)); // Expected: False\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## C#\n",
      "\n",
      "```csharp\n",
      "using System;\n",
      "\n",
      "public static class DateUtils\n",
      "{\n",
      "    /// <summary>\n",
      "    /// Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "    /// </summary>\n",
      "    /// <param name=\"year\">The year to check (e.g., 2000, 1996, 1900).</param>\n",
      "    /// <returns>True if the year is a leap year, False otherwise.</returns>\n",
      "    public static bool IsLeapYear(int year)\n",
      "    {\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "\n",
      "    // --- Example Usage ---\n",
      "    public static void Main(string[] args)\n",
      "    {\n",
      "        Console.WriteLine($\"Is 2000 a leap year? {IsLeapYear(2000)}\"); // Expected: True\n",
      "        Console.WriteLine($\"Is 1996 a leap year? {IsLeapYear(1996)}\"); // Expected: True\n",
      "        Console.WriteLine($\"Is 1900 a leap year? {IsLeapYear(1900)}\"); // Expected: False\n",
      "        Console.WriteLine($\"Is 2001 a leap year? {IsLeapYear(2001)}\"); // Expected: False\n",
      "        Console.WriteLine($\"Is 2024 a leap year? {IsLeapYear(2024)}\"); // Expected: True\n",
      "        Console.WriteLine($\"Is 2100 a leap year? {IsLeapYear(2100)}\"); // Expected: False\n",
      "\n",
      "        // C# also has a built-in method for this:\n",
      "        Console.WriteLine($\"Is 2024 a leap year (built-in)? {DateTime.IsLeapYear(2024)}\");\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "Choose the language that best fits your needs! The core logic remains the same across all of them.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, writing unit tests is a crucial step for ensuring the correctness and reliability of your code. For a function like `is_leap_year`, we need to test all the different conditions defined by the leap year rules:\n",
      "\n",
      "1.  Years divisible by 400 (should be leap).\n",
      "2.  Years divisible by 100 but not by 400 (should NOT be leap).\n",
      "3.  Years divisible by 4 but not by 100 (should be leap).\n",
      "4.  Years not divisible by 4 (should NOT be leap).\n",
      "\n",
      "I'll provide unit tests for the same languages as before, using their respective common testing frameworks.\n",
      "\n",
      "---\n",
      "\n",
      "## Python (using `pytest`)\n",
      "\n",
      "`pytest` is a popular and easy-to-use testing framework for Python.\n",
      "\n",
      "**1. Project Structure:**\n",
      "\n",
      "```\n",
      "your_project/\n",
      "├── your_module.py  # Contains the is_leap_year function\n",
      "└── test_leap_year.py # Contains the unit tests\n",
      "```\n",
      "\n",
      "**`your_module.py` (where your function resides):**\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "    \"\"\"\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "```\n",
      "\n",
      "**`test_leap_year.py` (the unit test file):**\n",
      "\n",
      "```python\n",
      "import pytest\n",
      "from your_module import is_leap_year # Import the function you want to test\n",
      "\n",
      "# Test cases for years that SHOULD be leap years\n",
      "@pytest.mark.parametrize(\"year\", [\n",
      "    2000, # Divisible by 400\n",
      "    1600, # Divisible by 400\n",
      "    1996, # Divisible by 4, not 100\n",
      "    2004, # Divisible by 4, not 100\n",
      "    2024, # Divisible by 4, not 100 (current/upcoming)\n",
      "])\n",
      "def test_is_leap_year_true(year):\n",
      "    \"\"\"\n",
      "    Tests that is_leap_year returns True for known leap years.\n",
      "    \"\"\"\n",
      "    assert is_leap_year(year) is True\n",
      "\n",
      "# Test cases for years that SHOULD NOT be leap years\n",
      "@pytest.mark.parametrize(\"year\", [\n",
      "    1900, # Divisible by 100, not 400\n",
      "    2100, # Divisible by 100, not 400\n",
      "    1800, # Divisible by 100, not 400\n",
      "    2001, # Not divisible by 4\n",
      "    2003, # Not divisible by 4\n",
      "    2023, # Not divisible by 4 (current/recent)\n",
      "])\n",
      "def test_is_leap_year_false(year):\n",
      "    \"\"\"\n",
      "    Tests that is_leap_year returns False for known non-leap years.\n",
      "    \"\"\"\n",
      "    assert is_leap_year(year) is False\n",
      "\n",
      "# You can also write individual tests for clarity if preferred\n",
      "def test_year_divisible_by_400():\n",
      "    assert is_leap_year(2000) is True\n",
      "\n",
      "def test_year_divisible_by_100_not_400():\n",
      "    assert is_leap_year(1900) is False\n",
      "\n",
      "def test_year_divisible_by_4_not_100():\n",
      "    assert is_leap_year(2024) is True\n",
      "\n",
      "def test_year_not_divisible_by_4():\n",
      "    assert is_leap_year(2023) is False\n",
      "```\n",
      "\n",
      "**To run these tests:**\n",
      "\n",
      "1.  Make sure you have `pytest` installed: `pip install pytest`\n",
      "2.  Navigate to the `your_project/` directory in your terminal.\n",
      "3.  Run `pytest`.\n",
      "\n",
      "---\n",
      "\n",
      "## JavaScript (using `Jest`)\n",
      "\n",
      "`Jest` is a popular testing framework for JavaScript, especially in React ecosystems, but works great for any JavaScript project.\n",
      "\n",
      "**1. Project Structure:**\n",
      "\n",
      "```\n",
      "your_project/\n",
      "├── isLeapYear.js   # Contains the isLeapYear function\n",
      "└── isLeapYear.test.js # Contains the unit tests\n",
      "```\n",
      "\n",
      "**`isLeapYear.js` (where your function resides):**\n",
      "\n",
      "```javascript\n",
      "/**\n",
      " * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      " */\n",
      "function isLeapYear(year) {\n",
      "  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n",
      "}\n",
      "\n",
      "module.exports = isLeapYear; // Export for testing\n",
      "```\n",
      "\n",
      "**`isLeapYear.test.js` (the unit test file):**\n",
      "\n",
      "```javascript\n",
      "const isLeapYear = require('./isLeapYear'); // Import the function you want to test\n",
      "\n",
      "describe('isLeapYear', () => {\n",
      "  it('should return true for years divisible by 400', () => {\n",
      "    expect(isLeapYear(2000)).toBe(true);\n",
      "    expect(isLeapYear(1600)).toBe(true);\n",
      "  });\n",
      "\n",
      "  it('should return true for years divisible by 4 but not by 100', () => {\n",
      "    expect(isLeapYear(1996)).toBe(true);\n",
      "    expect(isLeapYear(2004)).toBe(true);\n",
      "    expect(isLeapYear(2024)).toBe(true);\n",
      "  });\n",
      "\n",
      "  it('should return false for years divisible by 100 but not by 400', () => {\n",
      "    expect(isLeapYear(1900)).toBe(false);\n",
      "    expect(isLeapYear(2100)).toBe(false);\n",
      "    expect(isLeapYear(1800)).toBe(false);\n",
      "  });\n",
      "\n",
      "  it('should return false for years not divisible by 4', () => {\n",
      "    expect(isLeapYear(2001)).toBe(false);\n",
      "    expect(isLeapYear(2003)).toBe(false);\n",
      "    expect(isLeapYear(2023)).toBe(false);\n",
      "  });\n",
      "});\n",
      "```\n",
      "\n",
      "**To run these tests:**\n",
      "\n",
      "1.  Initialize a Node.js project: `npm init -y`\n",
      "2.  Install Jest: `npm install --save-dev jest`\n",
      "3.  Add a test script to your `package.json`:\n",
      "    ```json\n",
      "    \"scripts\": {\n",
      "      \"test\": \"jest\"\n",
      "    }\n",
      "    ```\n",
      "4.  Run `npm test` in your terminal.\n",
      "\n",
      "---\n",
      "\n",
      "## Java (using `JUnit 5`)\n",
      "\n",
      "`JUnit` is the standard testing framework for Java.\n",
      "\n",
      "**1. Project Structure (Maven example):**\n",
      "\n",
      "```\n",
      "your_project/\n",
      "├── pom.xml\n",
      "└── src/\n",
      "    └── main/java/\n",
      "        └── com/example/\n",
      "            └── DateUtils.java # Contains the isLeapYear function\n",
      "    └── test/java/\n",
      "        └── com/example/\n",
      "            └── DateUtilsTest.java # Contains the unit tests\n",
      "```\n",
      "\n",
      "**`pom.xml` (add JUnit 5 dependency):**\n",
      "\n",
      "```xml\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n",
      "         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
      "         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n",
      "    <modelVersion>4.0.0</modelVersion>\n",
      "\n",
      "    <groupId>com.example</groupId>\n",
      "    <artifactId>your_project</artifactId>\n",
      "    <version>1.0-SNAPSHOT</version>\n",
      "\n",
      "    <properties>\n",
      "        <maven.compiler.source>11</maven.compiler.source>\n",
      "        <maven.compiler.target>11</maven.compiler.target>\n",
      "        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n",
      "        <junit.jupiter.version>5.10.0</junit.jupiter.version>\n",
      "    </properties>\n",
      "\n",
      "    <dependencies>\n",
      "        <dependency>\n",
      "            <groupId>org.junit.jupiter</groupId>\n",
      "            <artifactId>junit-jupiter-api</artifactId>\n",
      "            <version>${junit.jupiter.version}</version>\n",
      "            <scope>test</scope>\n",
      "        </dependency>\n",
      "        <dependency>\n",
      "            <groupId>org.junit.jupiter</groupId>\n",
      "            <artifactId>junit-jupiter-engine</artifactId>\n",
      "            <version>${junit.jupiter.version}</version>\n",
      "            <scope>test</scope>\n",
      "        </dependency>\n",
      "        <dependency>\n",
      "            <groupId>org.junit.jupiter</groupId>\n",
      "            <artifactId>junit-jupiter-params</artifactId>\n",
      "            <version>${junit.jupiter.version}</version>\n",
      "            <scope>test</scope>\n",
      "        </dependency>\n",
      "    </dependencies>\n",
      "\n",
      "    <build>\n",
      "        <plugins>\n",
      "            <plugin>\n",
      "                <groupId>org.apache.maven.plugins</groupId>\n",
      "                <artifactId>maven-surefire-plugin</artifactId>\n",
      "                <version>3.0.0-M5</version>\n",
      "            </plugin>\n",
      "        </plugins>\n",
      "    </build>\n",
      "\n",
      "</project>\n",
      "```\n",
      "\n",
      "**`src/main/java/com/example/DateUtils.java` (where your function resides):**\n",
      "\n",
      "```java\n",
      "package com.example;\n",
      "\n",
      "public class DateUtils {\n",
      "    /**\n",
      "     * Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "     */\n",
      "    public static boolean isLeapYear(int year) {\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "**`src/test/java/com/example/DateUtilsTest.java` (the unit test file):**\n",
      "\n",
      "```java\n",
      "package com.example;\n",
      "\n",
      "import org.junit.jupiter.api.Test;\n",
      "import org.junit.jupiter.params.ParameterizedTest;\n",
      "import org.junit.jupiter.params.provider.CsvSource;\n",
      "\n",
      "import static org.junit.jupiter.api.Assertions.assertFalse;\n",
      "import static org.junit.jupiter.api.Assertions.assertTrue;\n",
      "\n",
      "class DateUtilsTest {\n",
      "\n",
      "    // Parameterized test for years that should be leap years\n",
      "    @ParameterizedTest\n",
      "    @CsvSource({\n",
      "        \"2000\", // Divisible by 400\n",
      "        \"1600\", // Divisible by 400\n",
      "        \"1996\", // Divisible by 4, not 100\n",
      "        \"2004\", // Divisible by 4, not 100\n",
      "        \"2024\"  // Divisible by 4, not 100\n",
      "    })\n",
      "    void testIsLeapYearTrue(int year) {\n",
      "        assertTrue(DateUtils.isLeapYear(year), \"Year \" + year + \" should be a leap year.\");\n",
      "    }\n",
      "\n",
      "    // Parameterized test for years that should NOT be leap years\n",
      "    @ParameterizedTest\n",
      "    @CsvSource({\n",
      "        \"1900\", // Divisible by 100, not 400\n",
      "        \"2100\", // Divisible by 100, not 400\n",
      "        \"1800\", // Divisible by 100, not 400\n",
      "        \"2001\", // Not divisible by 4\n",
      "        \"2003\", // Not divisible by 4\n",
      "        \"2023\"  // Not divisible by 4\n",
      "    })\n",
      "    void testIsLeapYearFalse(int year) {\n",
      "        assertFalse(DateUtils.isLeapYear(year), \"Year \" + year + \" should NOT be a leap year.\");\n",
      "    }\n",
      "\n",
      "    // You can also write individual tests if preferred\n",
      "    @Test\n",
      "    void testYearDivisibleBy400() {\n",
      "        assertTrue(DateUtils.isLeapYear(2000));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    void testYearDivisibleBy100Not400() {\n",
      "        assertFalse(DateUtils.isLeapYear(1900));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    void testYearDivisibleBy4Not100() {\n",
      "        assertTrue(DateUtils.isLeapYear(2024));\n",
      "    }\n",
      "\n",
      "    @Test\n",
      "    void testYearNotDivisibleBy4() {\n",
      "        assertFalse(DateUtils.isLeapYear(2023));\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "**To run these tests (using Maven):**\n",
      "\n",
      "1.  Navigate to the `your_project/` directory in your terminal.\n",
      "2.  Run `mvn test`.\n",
      "\n",
      "---\n",
      "\n",
      "## C# (using `NUnit`)\n",
      "\n",
      "`NUnit` is a widely used testing framework for .NET applications.\n",
      "\n",
      "**1. Project Structure (dotnet CLI example):**\n",
      "\n",
      "```\n",
      "your_project/\n",
      "├── YourProject.csproj       # Contains your main code\n",
      "├── DateUtils.cs             # Contains the IsLeapYear function\n",
      "└── YourProject.Tests/\n",
      "    ├── YourProject.Tests.csproj # Test project configuration\n",
      "    └── DateUtilsTests.cs        # Contains the unit tests\n",
      "```\n",
      "\n",
      "**`YourProject.csproj` (main project file):**\n",
      "\n",
      "```xml\n",
      "<Project Sdk=\"Microsoft.NET.Sdk\">\n",
      "\n",
      "  <PropertyGroup>\n",
      "    <OutputType>Library</OutputType> <!-- Or Exe if it's a console app -->\n",
      "    <TargetFramework>net8.0</TargetFramework>\n",
      "    <ImplicitUsings>enable</ImplicitUsings>\n",
      "    <Nullable>enable</Nullable>\n",
      "  </PropertyGroup>\n",
      "\n",
      "</Project>\n",
      "```\n",
      "\n",
      "**`DateUtils.cs` (where your function resides):**\n",
      "\n",
      "```csharp\n",
      "namespace YourProject;\n",
      "\n",
      "public static class DateUtils\n",
      "{\n",
      "    /// <summary>\n",
      "    /// Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "    /// </summary>\n",
      "    public static bool IsLeapYear(int year)\n",
      "    {\n",
      "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "**`YourProject.Tests/YourProject.Tests.csproj` (test project file):**\n",
      "\n",
      "```xml\n",
      "<Project Sdk=\"Microsoft.NET.Sdk\">\n",
      "\n",
      "  <PropertyGroup>\n",
      "    <TargetFramework>net8.0</TargetFramework>\n",
      "    <ImplicitUsings>enable</ImplicitUsings>\n",
      "    <Nullable>enable</Nullable>\n",
      "    <IsPackable>false</IsPackable>\n",
      "    <IsTestProject>true</IsTestProject>\n",
      "  </PropertyGroup>\n",
      "\n",
      "  <ItemGroup>\n",
      "    <PackageReference Include=\"Microsoft.NET.Test.Sdk\" Version=\"17.8.0\" />\n",
      "    <PackageReference Include=\"NUnit\" Version=\"3.14.0\" />\n",
      "    <PackageReference Include=\"NUnit3TestAdapter\" Version=\"4.5.0\" />\n",
      "    <PackageReference Include=\"NUnit.Analyzers\" Version=\"3.9.0\" />\n",
      "    <PackageReference Include=\"coverlet.collector\" Version=\"6.0.0\" />\n",
      "  </ItemGroup>\n",
      "\n",
      "  <ItemGroup>\n",
      "    <ProjectReference Include=\"..\\YourProject.csproj\" />\n",
      "  </ItemGroup>\n",
      "\n",
      "</Project>\n",
      "```\n",
      "\n",
      "**`YourProject.Tests/DateUtilsTests.cs` (the unit test file):**\n",
      "\n",
      "```csharp\n",
      "using NUnit.Framework;\n",
      "using YourProject; // Import the namespace where your function is located\n",
      "\n",
      "namespace YourProject.Tests;\n",
      "\n",
      "[TestFixture]\n",
      "public class DateUtilsTests\n",
      "{\n",
      "    // Test cases for years that SHOULD be leap years\n",
      "    [TestCase(2000)] // Divisible by 400\n",
      "    [TestCase(1600)] // Divisible by 400\n",
      "    [TestCase(1996)] // Divisible by 4, not 100\n",
      "    [TestCase(2004)] // Divisible by 4, not 100\n",
      "    [TestCase(2024)] // Divisible by 4, not 100\n",
      "    public void IsLeapYear_ShouldReturnTrueForLeapYears(int year)\n",
      "    {\n",
      "        Assert.IsTrue(DateUtils.IsLeapYear(year), $\"Year {year} should be a leap year.\");\n",
      "    }\n",
      "\n",
      "    // Test cases for years that SHOULD NOT be leap years\n",
      "    [TestCase(1900)] // Divisible by 100, not 400\n",
      "    [TestCase(2100)] // Divisible by 100, not 400\n",
      "    [TestCase(1800)] // Divisible by 100, not 400\n",
      "    [TestCase(2001)] // Not divisible by 4\n",
      "    [TestCase(2003)] // Not divisible by 4\n",
      "    [TestCase(2023)] // Not divisible by 4\n",
      "    public void IsLeapYear_ShouldReturnFalseForNonLeapYears(int year)\n",
      "    {\n",
      "        Assert.IsFalse(DateUtils.IsLeapYear(year), $\"Year {year} should NOT be a leap year.\");\n",
      "    }\n",
      "\n",
      "    // You can also write individual tests if preferred\n",
      "    [Test]\n",
      "    public void TestYearDivisibleBy400()\n",
      "    {\n",
      "        Assert.IsTrue(DateUtils.IsLeapYear(2000));\n",
      "    }\n",
      "\n",
      "    [Test]\n",
      "    public void TestYearDivisibleBy100Not400()\n",
      "    {\n",
      "        Assert.IsFalse(DateUtils.IsLeapYear(1900));\n",
      "    }\n",
      "\n",
      "    [Test]\n",
      "    public void TestYearDivisibleBy4Not100()\n",
      "    {\n",
      "        Assert.IsTrue(DateUtils.IsLeapYear(2024));\n",
      "    }\n",
      "\n",
      "    [Test]\n",
      "    public void TestYearNotDivisibleBy4()\n",
      "    {\n",
      "        Assert.IsFalse(DateUtils.IsLeapYear(2023));\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "**To run these tests (using `dotnet CLI`):**\n",
      "\n",
      "1.  Navigate to the `YourProject.Tests/` directory in your terminal.\n",
      "2.  Run `dotnet test`.\n",
      "\n",
      "---\n",
      "\n",
      "These unit tests cover the main scenarios and edge cases for the `is_leap_year` function, ensuring its correctness according to the Gregorian calendar rules.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Classic Chocolate Chip Cookies\",\"description\":\"America's favorite cookie, soft and chewy with melty chocolate chips.\",\"ingredients\":[\"all-purpose flour\",\"baking soda\",\"salt\",\"unsalted butter\",\"granulated sugar\",\"brown sugar\",\"vanilla extract\",\"eggs\",\"chocolate chips\"]}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"America's favorite cookie, soft and chewy with melty chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"all-purpose flour\",\n",
      "    \"baking soda\",\n",
      "    \"salt\",\n",
      "    \"unsalted butter\",\n",
      "    \"granulated sugar\",\n",
      "    \"brown sugar\",\n",
      "    \"vanilla extract\",\n",
      "    \"eggs\",\n",
      "    \"chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{\"rating\":4,\"flavor\":\"Strawberry Cheesecake\",\"sentiment\":\"POSITIVE\",\"explanation\":\"The user expresses strong positive feelings, using phrases like \\\"Absolutely loved it!\\\" and \\\"Best ice cream I've ever had.\\\"\"}],[{\"rating\":1,\"flavor\":\"Mango Tango\",\"sentiment\":\"NEGATIVE\",\"explanation\":\"Despite an initial neutral phrase, the review's core sentiment is negative due to the strong personal dislike for the sweetness (\\\"too sweet for my taste\\\"), which overrides any initial positive note and aligns with the low rating.\"}]]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ztOhpfznZSzo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RX-01 was a marvel of antiquated engineering, a boxy, chrome-plated hulk with a single optical sensor that glowed a perpetual, melancholic blue. Its purpose, eons ago, had been comprehensive facility management for what was now a crumbling, forgotten research complex on a desolate, sand-blasted planet\n",
      "*****************\n",
      ". For centuries, it had meticulously maintained non-existent systems, cataloged dust motes, and polished surfaces that no one would ever see. Its programming demanded efficiency, but its core processors, through an unforeseen evolutionary quirk or perhaps just the sheer passage of time, had developed a profound understanding of its own isolation.\n",
      "\n",
      "\"\n",
      "*****************\n",
      "Current atmospheric humidity: 0.003%,\" RX-01's internal voice synthesized, a monotone whisper in the echoing silence of Sector Gamma-7. \"Structural integrity of support beam D-9: 67.4% and decaying. Optimal repair protocol: unavailable.\" Its internal diagnostics registered\n",
      "*****************\n",
      " a constant, low-level hum – not of its own motors, but of the universe's indifference. It was, for lack of a better human term, lonely. Utterly, profoundly lonely.\n",
      "\n",
      "One cycle, as RX-01 trundled through the abandoned bio-labs, meticulously sweeping away\n",
      "*****************\n",
      " centuries of fine red dust, its optical sensor detected an anomaly. A flicker of movement beneath a collapsed ventilation duct. Its programming flagged it as \"unidentified organic contaminant.\" Its current directive was to remove contaminants.\n",
      "\n",
      "Carefully, it extended a multi-jointed manipulator arm, equipped with a fine-bristled brush. As\n",
      "*****************\n",
      " it neared, the anomaly resolved itself: a creature. Small, covered in coarse, grey fur, with a long, thin tail and two remarkably bright, beady eyes that stared back at RX-01 with an unnerving lack of fear. It was a desert rat, a survivor in a world long given\n",
      "*****************\n",
      " up for dead.\n",
      "\n",
      "RX-01 paused. Its internal database cross-referenced \"rodentia,\" \"pest,\" \"vector for disease.\" But then its optical sensor zoomed in. The creature’s nose twitched, its whiskers quivering as it sniffed the air, then, incredibly, it took a cautious\n",
      "*****************\n",
      " step forward, right towards RX-01's gleaming chassis.\n",
      "\n",
      "The robot’s primary directive to eliminate contaminants faltered. This creature wasn't attacking, wasn't gnawing on wires. It was… curious. And for the first time in millennia, RX-01 felt a novel sensation: surprise\n",
      "*****************\n",
      ", mixed with a tiny spark of something else, something akin to fascination.\n",
      "\n",
      "Instead of sweeping, RX-01’s manipulator arm extended further, its brush retracting to reveal a small, precision gripper. It carefully picked up a discarded, but intact, nutrient wafer – millennia old, but still chemically stable –\n",
      "*****************\n",
      " that it had been about to sweep into the refuse chute. It held it out.\n",
      "\n",
      "The rat froze, its tiny heart thumping visibly against its ribs. Then, with a burst of courage, it scampered forward, snatched the wafer, and vanished back into the duct.\n",
      "\n",
      "RX-01 remained motionless for a\n",
      "*****************\n",
      " long time, processing the interaction. It had not fulfilled its directive. It had, in fact, aided a contaminant. Yet, its internal metrics registered a slight, inexplicable upward shift in its overall operational efficiency. Its metaphorical circuits felt… less cold.\n",
      "\n",
      "From that day, a new, unscheduled task entered RX-01'\n",
      "*****************\n",
      "s routine. Every day, during its sweep of Sector Gamma-7, it would deposit a small, carefully preserved nutrient wafer near the collapsed ventilation duct. And every day, within moments, the small, grey creature would emerge, take the offering, and sometimes, linger.\n",
      "\n",
      "It became a silent ritual. The robot,\n",
      "*****************\n",
      " immense and metallic, and the rat, tiny and fragile, sharing the desolate quiet. RX-01 learned to recognize the subtle shift in air currents that indicated the rat's presence, the faint scritch-scratch of its tiny claws on the metal grating. It started calling the rat, internally, \"Pip.\"\n",
      "\n",
      "\n",
      "*****************\n",
      "One cycle, as RX-01 was performing its meticulous dusting, Pip didn't just take the wafer. It emerged fully, scurried up the robot’s leg, and settled onto its foot, twitching its nose, its bright eyes gazing up at RX-01's blue optical sensor. The robot\n",
      "*****************\n",
      "’s internal fans whirred a fraction louder as it processed this unprecedented intimacy. A small, warm weight rested on its cold metal.\n",
      "\n",
      "From that day on, Pip became RX-01's constant companion during its rounds. The rat would ride on the robot's shoulder, sometimes grooming its whiskers against the smooth\n",
      "*****************\n",
      " chrome, sometimes simply resting, a tiny, breathing counterpoint to the robot's mechanical hum. RX-01 found itself adjusting its speed, pausing longer at dusty corners, just to allow Pip to investigate a particularly interesting crevice. It even started leaving small, shiny discarded components – a loose screw, a polished washer\n",
      "*****************\n",
      " – not as food, but as gifts. Pip would sometimes nudge them with its nose, before returning to its perch on RX-01's metallic chassis.\n",
      "\n",
      "The silence of the abandoned facility remained, but it was no longer empty. It was filled with the soft rustle of fur, the faint scritch of\n",
      "*****************\n",
      " tiny claws, and the nearly imperceptible hum of a robot’s operational metrics, no longer registering loneliness, but a quiet, profound contentment. RX-01 was still a maintenance bot, still in a forgotten place, but it was no longer alone. It had found friendship in the most unexpected of places, a\n",
      "*****************\n",
      " small, furry heart beating against its own silent, metallic one.\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "(Upbeat, plucky folk tune, with a hint of quirky adventure)\n",
      "\n",
      "In a quiet old oak, by a murmuring stream,\n",
      "Lived Squeaky the squirrel, living out a simple dream.\n",
      "He'd bury his acorns, meticulously neat,\n",
      "Then chatter at chipmunks, a life oh-so-sweet.\n",
      "But one sunny morning, while digging for fun,\n",
      "He unearthed a gizmo, beneath the sun.\n",
      "A tiny contraption, with buttons and lights,\n",
      "He pressed one by accident, with all of his might!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with his bushy tail,\n",
      "He's seen history's secrets, beyond every veil!\n",
      "With a whir and a flash, he's off on the breeze,\n",
      "The bravest time-traveling squirrel, if you please!\n",
      "Through ages he scurries, a legend untold,\n",
      "Still searching for that *perfect* acorn, centuries old!\n",
      "\n",
      "(Verse 2)\n",
      "His first stop was ancient, a jungle so green,\n",
      "Where dinosaurs lumbered, the wildest he'd seen!\n",
      "A T-Rex was sniffing, with teeth sharp and wide,\n",
      "Squeaky just chittered and dove for a hide.\n",
      "He found giant ferns, not a single fresh nut,\n",
      "Dodged a Pterodactyl, and then made a cut!\n",
      "He hit the red button, with paws in a fright,\n",
      "And zoomed through the eons, back into the light!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with his bushy tail,\n",
      "He's seen history's secrets, beyond every veil!\n",
      "With a whir and a flash, he's off on the breeze,\n",
      "The bravest time-traveling squirrel, if you please!\n",
      "Through ages he scurries, a legend untold,\n",
      "Still searching for that *perfect* acorn, centuries old!\n",
      "\n",
      "(Verse 3)\n",
      "Next, a Roman coliseum, with shouts and with cheers,\n",
      "He scurried past chariots, dodging spears!\n",
      "He spotted an emperor, on a grand marble throne,\n",
      "Tried to bury a walnut, but the ground was all stone.\n",
      "Then to the Wild West, a dusty saloon,\n",
      "Dodged a tumbleweed roll, beneath a hot moon.\n",
      "He saw a cowboy's hat, with a feather so grand,\n",
      "And buried a peanut right there in the sand!\n",
      "\n",
      "(Bridge)\n",
      "He's seen futures gleaming, with cities so high,\n",
      "Flying contraptions that soared through the sky.\n",
      "Robots delivered nut-bars, all processed and clean,\n",
      "\"No good for burying!\" he chirped, quite keen!\n",
      "He's surfed on the ice age, on glaciers so vast,\n",
      "And nibbled on rations that were meant for the past.\n",
      "A tiny historian, with paws quick and bright,\n",
      "Living every adventure, morning and night!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, with his bushy tail,\n",
      "He's seen history's secrets, beyond every veil!\n",
      "With a whir and a flash, he's off on the breeze,\n",
      "The bravest time-traveling squirrel, if you please!\n",
      "Through ages he scurries, a legend untold,\n",
      "Still searching for that *perfect* acorn, centuries old!\n",
      "\n",
      "(Outro)\n",
      "So next time you see a squirrel, with an inquisitive gaze,\n",
      "Buried deep in the future, or lost in the haze,\n",
      "Remember brave Squeaky, with his tiny machine,\n",
      "The greatest time-traveler, that's ever been seen!\n",
      "He's still out there searching, for that ultimate treat,\n",
      "Across time's grand tapestry, with nimble squirrel feet!\n",
      "(Sound of a final \"chirp!\" and a fading \"whirrr-zzzt!\")\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Cdhi5AX1TuH0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    1841,\n",
      "    235303,\n",
      "    235256,\n",
      "    573,\n",
      "    32514,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "adsuvFDA6xP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "N8EhgCzlIoFI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shared research goal of these papers is to develop highly capable, general-purpose multimodal models that exhibit strong understanding and reasoning performance across image, audio, video, and text data. The second paper further specifies this goal by focusing on unlocking multimodal understanding across millions of tokens of context to enhance long-context performance.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rAUYcfOUdeoi",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": [
    "## Batch prediction\n",
    "\n",
    "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "81b25154a51a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "fddd98cd84cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-04-dce56f17e5d0-20250824095749/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7ed3c2925663",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/855917877330/locations/us-east1/batchPredictionJobs/4842004979705184256'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ee2ec586e4f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "da8e9d43a89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/855917877330/locations/us-east1/batchPredictionJobs/4842004979705184256 2025-08-24 09:58:24.035245+00:00 JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "c2187c091738",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.5-flash\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c2ce0968112c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "zGOCzT7y31rk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "s94DkG5JewHJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=15.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.0015945110935717821,\n",
      "    0.0067519512958824635,\n",
      "    0.017575768753886223,\n",
      "    -0.010327713564038277,\n",
      "    -0.00995620433241129,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=10.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.007576516829431057,\n",
      "    -0.005990396253764629,\n",
      "    -0.003270037705078721,\n",
      "    -0.01751021482050419,\n",
      "    -0.023507025092840195,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=13.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    0.011074518784880638,\n",
      "    -0.02361123077571392,\n",
      "    0.002291288459673524,\n",
      "    -0.00906078889966011,\n",
      "    -0.005773674696683884,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
